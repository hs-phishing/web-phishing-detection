# -*- coding: utf-8 -*-
"""merge_feature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WrrtzMWX-RBm6Qhy9EqFDP22ei9zIvwv
"""

import pandas as pd
import concurrent.futures

# 테스트 데이터 셋
df = pd.read_csv('test_dataset.csv')

# pip install python-whois

# 라이브러리 임포트
import requests
import regex # pip install regex
import ipaddress # pip install ipaddress
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import url_based_feature as ubf
import content_based_features as cbf
import domainver1 as dbf
import short_url_features as suf

# # 병렬 처리를 위해 ThreadPoolExecutor 설정
# def apply_function_in_parallel(df, func_name, col_name):
#     def wrapper(url):
#         result = func_name(url)
#         print(f"{url} - {col_name} 피처 완료")
#         return result

#     with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
#         results = list(executor.map(wrapper, df['url']))

#     df[col_name] = results
#     df.reset_index(drop=True, inplace=True)
#     if col_name in df.columns:
#         print(f"피처 '{col_name}' 추가 완료:")
#         print(df[[col_name]].head())
#     else:
#         print(f"피처 '{col_name}'이 데이터프레임에 존재하지 않습니다.")

def apply(df, func_name, col_name):
    results = []

    # check_url이 이 파일에 정의된 함수라면, 직접 호출
    for url in df['url']:
        # 단축 URL 판별 및 복원
        # expanded_url = suf.check_url(url)  # 직접 호출하여 URL 복원
        result = func_name(url)  # 변환된 URL을 func_name 함수에 전달하여 결과 생성
        results.append(result)  # 결과 리스트에 추가

    df[col_name] = results  # 결과를 새로운 컬럼에 추가
    df.reset_index(drop=True, inplace=True)  # 인덱스 재설정

    if col_name not in df.columns:
        print(f"피처 '{col_name}'이 데이터프레임에 존재하지 않습니다.")

apply(df, cbf.use_right_click, 'RightClick')

apply(df, cbf.popup_window_text, 'popUpWidnow')

apply(df, cbf.iFrame_redirection, 'Iframe')

apply(df, cbf.using_ip, 'having_IPhaving_IP_Address')

apply(df, cbf.check_favicon, 'Favicon')

apply(df, cbf.check_request_url, 'Request_URL')

apply(df, cbf.check_url_of_anchor, 'URL_of_Anchor')

apply(df, cbf.has_meta_tags, 'Links_in_tags')

apply(df, cbf.check_sfh, 'SFH')

apply(df, cbf.check_submit_email, 'Submitting_to_email')

apply(df, cbf.check_redirect_count, 'Redirect')

apply(df, cbf.check_onmouseover_change, 'on_mouseover')

apply(df, ubf.check_url_length, 'URLURL_Length')

apply(df, ubf.port_scan, 'port')

apply(df, ubf.check_at_symbol, 'having_At_Symbol')

apply(df, ubf.check_double_slash_redirecting, 'double_slash_redirecting')

apply(df, ubf.check_prefix_suffix, 'Prefix_Suffix')

apply(df, ubf.check_abnormal_url, ' Abnormal_URL')

apply(df, dbf.google_index, ' Google_Index')

apply(df, dbf.domain_age, ' age_of_domain')

apply(df, dbf.dns_record, ' DNSRecord')

apply(df, dbf.domain_registration_period, ' Domain_registeration_length')

apply(df, dbf.ssl_certificate_status, ' SSLfinal_State')

apply(df, dbf.having_sub_domain, ' having_Sub_Domain')

apply(df, dbf.https_token, ' HTTPS_token')

# apply(df, dbf.web_traffic, 'web_traffic')
# apply(df, dbf.page_rank, 'Page_Rank')
# apply(df, dbf.links_pointing_to_page, 'Links_pointing_to_page')
# apply(df, dbf.statistical_report, 'Statistical_report')

# 결과를 CSV 파일로 저장
df.to_csv('merge_test_dataset.csv', index=False)